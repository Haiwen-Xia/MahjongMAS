import torch
import torch.multiprocessing as mp # ä¸ºæ›´å¥½åœ°å¤„ç†CUDAï¼ˆå¦‚æœå­è¿›ç¨‹ä¹Ÿä½¿ç”¨CUDAï¼‰ï¼Œä½¿ç”¨torchçš„å¤šè¿›ç¨‹
import os
import logging
import json
import time # ç”¨äºjoinè¶…æ—¶å’Œå¯èƒ½çš„sleep
import signal # ç”¨äºä¿¡å·å¤„ç†
import sys # ç”¨äºsys.exit

# å‡è®¾è¿™äº›å¯¼å…¥è·¯å¾„ç›¸å¯¹äºæ‚¨çš„é¡¹ç›®ç»“æ„æ˜¯æ­£ç¡®çš„
from replay_buffer import ReplayBuffer 
from actor.actor import Actor # Actor ç±» 
from learner.learner import Learner # Learner ç±» 
from inference_server.inference_server import InferenceServer # å¯¼å…¥æ–°çš„ InferenceServer
from utils import setup_process_logging_and_tensorboard, save_experiment_config # æ—¥å¿—å’Œ TensorBoard è®¾ç½®å·¥å…·

from models.actor import ResNet34Actor # å¯¼å…¥å…·ä½“çš„ Actor æ¨¡å‹
from models.critic import ResNet34CentralizedCritic # å¯¼å…¥å…·ä½“çš„ Critic æ¨¡å‹

from agent.feature import FeatureAgent # å¯¼å…¥ FeatureAgent ç±»

# --- å…¨å±€å…³é—­äº‹ä»¶å’Œè¿›ç¨‹å¥æŸ„ï¼Œç”¨äºä¿¡å·å¤„ç† ---
shutdown_event = mp.Event() # ç”¨äºé€šçŸ¥æ‰€æœ‰å­è¿›ç¨‹å¼€å§‹å…³é—­çš„äº‹ä»¶
g_learner_process = None    # Learner è¿›ç¨‹çš„å…¨å±€å¥æŸ„
g_actor_processes = []      # Actor è¿›ç¨‹åˆ—è¡¨çš„å…¨å±€å¥æŸ„
g_inference_server_process = None # InferenceServer è¿›ç¨‹çš„å…¨å±€å¥æŸ„
g_main_logger = None        # ä¸»è¿›ç¨‹çš„ logger
g_main_writer = None        # ä¸»è¿›ç¨‹çš„ TensorBoard writer
# é˜Ÿåˆ—ä¹Ÿéœ€è¦å…¨å±€å¯è®¿é—®ï¼Œä»¥ä¾¿ cleanup å‡½æ•°å¯ä»¥å‘é€ SHUTDOWN å‘½ä»¤
g_learner_to_server_cmd_q = None


def cleanup_all_processes(signal_received=None, frame=None):
    """
    é›†ä¸­çš„æ¸…ç†å‡½æ•°ï¼Œç”¨äºå…³é—­æ‰€æœ‰å­è¿›ç¨‹å’Œé‡Šæ”¾ç›¸å…³èµ„æºã€‚
    å¯ä»¥ç”±ä¿¡å·å¤„ç†å™¨æˆ–ä¸»ç¨‹åºçš„ finally å—è°ƒç”¨ã€‚
    """
    global shutdown_event, g_learner_process, g_actor_processes, g_inference_server_process
    global g_main_logger, g_main_writer, g_learner_to_server_cmd_q
    
    # é˜²æ­¢é‡å…¥ (å¦‚æœä¿¡å·å¤„ç†å™¨è¢«å¤šæ¬¡è§¦å‘æˆ– cleanup è¢«å¤šæ¬¡è°ƒç”¨)
    if hasattr(cleanup_all_processes, 'is_shutting_down') and cleanup_all_processes.is_shutting_down:
        return
    cleanup_all_processes.is_shutting_down = True # è®¾ç½®æ ‡å¿—ï¼Œè¡¨ç¤ºæ­£åœ¨å…³é—­

    log_func = print # é»˜è®¤ä½¿ç”¨ printï¼Œå¦‚æœ logger å¯ç”¨åˆ™ä½¿ç”¨ logger
    if g_main_logger:
        log_func = g_main_logger.warning if signal_received else g_main_logger.info

    if signal_received:
        log_func(f"Main process received signal {signal.Signals(signal_received).name}. Initiating graceful shutdown...")
    else:
        log_func("Main process initiating cleanup (e.g., from finally block or normal exit)...")

    log_func("Setting shutdown event for child processes...")
    shutdown_event.set() # é€šçŸ¥æ‰€æœ‰å­è¿›ç¨‹

    # 1. ç­‰å¾… Actor è¿›ç¨‹é€€å‡º (å®ƒä»¬å¯èƒ½ä¾èµ– InferenceServerï¼Œä½†åº”é¦–å…ˆåœæ­¢å…¶å¾ªç¯)
    for actor_proc in g_actor_processes:
        if actor_proc and actor_proc.is_alive():
            log_func(f"Waiting for Actor process ({actor_proc.name}, PID: {actor_proc.pid}) to join...")
            actor_proc.join(timeout=15) # æ¯ä¸ª actor ç­‰å¾…15ç§’
            if actor_proc.is_alive():
                log_func(f"Actor process ({actor_proc.name}) did not exit gracefully. Terminating...")
                actor_proc.terminate() # å¼ºåˆ¶ç»ˆæ­¢
                actor_proc.join(timeout=5) # ç­‰å¾…å¼ºåˆ¶ç»ˆæ­¢å®Œæˆ
            if actor_proc.is_alive(): # å†æ¬¡æ£€æŸ¥
                 log_func(f"Actor process ({actor_proc.name}) could not be terminated.")
            else:
                 log_func(f"Actor process ({actor_proc.name}) finished.")
    
    # 2. ç­‰å¾… Learner è¿›ç¨‹é€€å‡º (å®ƒå¯èƒ½éœ€è¦å‘ InferenceServer å‘é€æœ€åæ›´æ–°)
    if g_learner_process and g_learner_process.is_alive():
        log_func(f"Waiting for Learner process ({g_learner_process.name}, PID: {g_learner_process.pid}) to join...")
        g_learner_process.join(timeout=30) # ç»™ Learner 30ç§’æ—¶é—´
        if g_learner_process.is_alive():
            log_func(f"Learner process ({g_learner_process.name}) did not exit gracefully. Terminating...")
            g_learner_process.terminate()
            g_learner_process.join(timeout=5)
        if g_learner_process.is_alive():
            log_func(f"Learner process ({g_learner_process.name}) could not be terminated.")
        else:
            log_func(f"Learner process ({g_learner_process.name}) finished.")

    # 3. é€šçŸ¥ InferenceServer å…³é—­å¹¶ç­‰å¾…å…¶é€€å‡º
    if g_inference_server_process and g_inference_server_process.is_alive():
        log_func("Sending SHUTDOWN command to InferenceServer...")
        if g_learner_to_server_cmd_q: # ç¡®ä¿å‘½ä»¤é˜Ÿåˆ—å­˜åœ¨
            try:
                # Learner é€šå¸¸ä¼šå‘é€è¿™ä¸ªå‘½ä»¤ï¼Œä½†ä½œä¸ºå¤‡ç”¨ï¼Œä¸»è¿›ç¨‹ä¹Ÿå¯ä»¥å‘é€
                g_learner_to_server_cmd_q.put(("SHUTDOWN", None), timeout=5) # å¸¦è¶…æ—¶çš„ put
            except Exception as e_cmd_q_put: # ä¾‹å¦‚ queue.Full
                log_func(f"Error sending SHUTDOWN to InferenceServer via queue: {e_cmd_q_put}")
        
        log_func(f"Waiting for InferenceServer process ({g_inference_server_process.name}, PID: {g_inference_server_process.pid}) to join...")
        g_inference_server_process.join(timeout=20) # ç»™æœåŠ¡å™¨20ç§’æ—¶é—´å¤„ç†å…³é—­
        if g_inference_server_process.is_alive():
            log_func(f"InferenceServer process ({g_inference_server_process.name}) did not exit gracefully. Terminating...")
            g_inference_server_process.terminate()
            g_inference_server_process.join(timeout=5)
        if g_inference_server_process.is_alive():
            log_func(f"InferenceServer process ({g_inference_server_process.name}) could not be terminated.")
        else:
            log_func(f"InferenceServer process ({g_inference_server_process.name}) finished.")
    
    # 4. æ¸…ç†ä¸»è¿›ç¨‹çš„èµ„æº (ä¾‹å¦‚ TensorBoard writer)
    if g_main_writer:
        log_func("Main process closing its TensorBoard writer.")
        try: g_main_writer.close()
        except Exception as e_writer_close: log_func(f"Error closing main TensorBoard writer: {e_writer_close}")
    
    log_func("Main process cleanup finished.")
    if signal_received: # å¦‚æœæ˜¯ç”±ä¿¡å·è§¦å‘çš„ï¼Œåˆ™ä»¥ç›¸åº”æ–¹å¼é€€å‡º
        sys.exit(128 + signal_received) # å¸¸è§çš„Unixé€€å‡ºç çº¦å®š

# --- Configuration ---
# It's often better to load config from a file (e.g., YAML, JSON) or use argparse
CONFIG = {
    # å®éªŒå…ƒæ•°æ®
    'experiment_meta': {
        'experiment_name': "Using_Inference_Server", # Use underscores or avoid special chars for dir names
        'log_base_dir': 'logs', # Base directory for logs and TensorBoard
        'checkpoint_base_dir': 'log/model', # Base directory for checkpoints
    },
    
    # Model & Agent è®¾ç½®
    'model_agent_config' : {
        'in_channels': 187,
        'out_channels': 235,
        'critic_extra_in_channels': 16,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'actor_class': ResNet34Actor, # æŒ‡å®š Actor æ¨¡å‹ç±»
        'critic_class': ResNet34CentralizedCritic, # æŒ‡å®š Critic æ¨¡
        'agent_clz': FeatureAgent, # æŒ‡å®šç¯å¢ƒå†…éƒ¨ä½¿ç”¨çš„ Agent ç±»
        'initial_actor_eval_path': "initial_models/actor_from_sl.pth",
        'initial_critic_eval_path': "initial_models/centralized_critic_initialized.pth",
    },

    # Replay Buffer è®¾ç½®
    'replay_buffer_size': 50000,
    'replay_buffer_episode_capacity': 400, # Renamed for clarity

    # Benchmark ç­–ç•¥è®¾ç½®
    'benchmark_policies': {
        'initial_il_policy': "initial_models/actor_from_sl.pth",
    },
    'server_hosted_benchmark_names': ["initial_il_policy"],

    # å…³äº inference_server çš„ç›¸å…³è®¾ç½®
    'inference_server_config': {
        'inference_batch_size': 128,
        'inference_max_wait_ms': 5, # å•ä½ ms
        'max_history_actors': 10, # å†å² Actor çš„æœ€å¤§æ•°é‡
        'history_actors_update_every': 100, # æ¯ 100 æ¬¡æ›´æ–°æ¨¡å‹ä¿å­˜ä¸€æ¬¡å†å² Actor æ¨¡å‹
    },

    # åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
    'num_actors': 8,

    # å…³äº Actor çš„ç›¸å…³è®¾ç½®
    'actor_config': {
        'log_interval': 10, # Actor æ—¥å¿—é—´éš”
        'num_env_steps': 10000000, # Total number of environment steps to run across all actors
        'num_envs_per_actor': 8, # æ¯ä¸ª Actor è¿è¡Œçš„ç¯å¢ƒæ•°é‡
        # å¯¹æ‰‹çš„ç›¸å…³æ¶‰èµ„

        # å¤šæ ·åŒ– opponent
        'p_opponent_historical' : 0.05,
        'p_opponent_benchmark': 1,
        'opponent_model_change_interval': 500, # æ¯å¤šå°‘ä¸ª episode æ›¿æ¢ä¸€æ¬¡å¯¹æ‰‹

        # æ”¶é›†æ•°æ®å¤„ç†
        'filter_single_action_steps': False, # [Deprecated] æ˜¯å¦è¿‡æ»¤æ‰åªæœ‰å•ä¸ªå¯èƒ½ action çš„æ—¶é—´æ­¥
        'use_normalized_reward': True,
        'draw_reward': -0.5,

        'inference_timeout_seconds': 5, 
    },

    # å…³äº Learner çš„ç›¸å…³è®¾ç½®
    'learner_config': {
        'log_interval': 100, 
        'model_push_interval': 10,
        'ckpt_save_interval_seconds': 600, # ä¿å­˜æ£€æŸ¥ç‚¹çš„é—´éš”æ—¶é—´
        'min_sample_to_start_learner': 20, # å¼€å§‹è®­ç»ƒéœ€è¦çš„ buffer æ ·æœ¬æ•°
        'training_components_log_freq': 1000,  # è®­ç»ƒç»„ä»¶çŠ¶æ€æ—¥å¿—é¢‘ç‡
    },

    # PPO åŸºæœ¬è®¾ç½®
    'ppo_config': {
        'gamma': 0.98,      # Discount factor for GAE/TD Target
        'lambda': 0.97,     # Lambda for GAE
        'clip': 0.2,        # PPO clip epsilon
        'grad_clip_norm': 0.3,
        'value_coeff': 0.5, # Coefficient for value loss (common to scale down)
        'entropy_coeff': -1e-3, # Coefficient for entropy bonus æ­£å¸¸æ˜¯æ­£æ•°ï¼Œä½†è¿™é‡Œæ˜¯è´Ÿæ•°è¡¨ç¤ºæƒ©ç½š
        'batch_size': 1024, # Increased batch size
        'epochs_per_batch': 5, # Renamed 'epochs' for clarity (PPO inner loops)
        'normalize_adv': True,


        # Learner Hyperparameters        'lr_actor': 3e-5,  # Actor overall learning rate
        'lr_critic_feature_extractor': 3e-4,  # Critic feature_extractor_obs learning rate  
        'lr_critic_head': 3e-4,  # Critic head (includes feature_extractor_extra + critic_head_mlp) learning rate
        
        # Learning rate scheduling
        'use_lr_scheduler': True,  # Enable learning rate scheduling
        "warmup_iterations": 1000,
        'total_iterations_for_lr_decay': 500000,
        'min_lr_for_scheduled_components': 1e-6,
        'initial_lr_warmup_actor': 3e-6,
        'initial_lr_warmup_critic_fe': 3e-6,
        'initial_lr_warmup_critic_head': 3e-5,
        # Staged training configuration
        'stage1_iterations': 1000,  # Only train critic_head group (fe_extra + head_mlp)
        'stage2_iterations': 2000,  # Unfreeze critic_fe_obs, keep actor frozen
        # Stage 3 starts at stage2_iterations: joint training of all components
    },

    # æ€§èƒ½ profiling ç›¸å…³è®¾ç½®
    'profiling_config': {
        'enable_profiling': False,
        'enable_profiling_actor': False
    }
}


def main():
    global g_learner_process, g_actor_processes, g_inference_server_process
    global g_main_logger, g_main_writer, shutdown_event, g_learner_to_server_cmd_q

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [ROOT/%(levelname)s] %(message)s', # Differentiate root logs
        handlers=[logging.StreamHandler()] # Root logs to console
    )
    
    run_name = CONFIG['experiment_meta']['experiment_name']
    log_base_dir = CONFIG['experiment_meta']['log_base_dir']  #logs

    try:
        # ä½¿ç”¨æ–°çš„æ—¥å¿—è®¾ç½®å‡½æ•°
        g_main_logger, g_main_writer, main_log_paths = setup_process_logging_and_tensorboard(
            log_base_dir, CONFIG, process_name='main_train', log_type='main'
        )
        
        # ä¿å­˜å®éªŒé…ç½®
        save_experiment_config(CONFIG, main_log_paths['config_save_path'])
        
    except Exception as e_log_setup:
        logging.error(f"Main process logger/writer setup failed: {e_log_setup}", exc_info=True)
        g_main_logger = logging.getLogger("main_fallback_logger") # ä½¿ç”¨ä¸€ä¸ªå¤‡ç”¨loggerï¼Œä»¥é˜²ä¸‡ä¸€
        g_main_writer = None # Writer å¯èƒ½æ— æ³•åˆ›å»º
        main_log_paths = {'checkpoint_dir': 'log/model'}  # æä¾›é»˜è®¤è·¯å¾„

    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    original_sigint_handler = signal.getsignal(signal.SIGINT) # ä¿å­˜åŸå§‹çš„ SIGINT å¤„ç†å™¨
    original_sigterm_handler = signal.getsignal(signal.SIGTERM) # ä¿å­˜åŸå§‹çš„ SIGTERM å¤„ç†å™¨
    try:
        signal.signal(signal.SIGINT, cleanup_all_processes)  # æ•è· Ctrl+C
        signal.signal(signal.SIGTERM, cleanup_all_processes) # æ•è· kill å‘½ä»¤
        g_main_logger.info("Main process signal handlers set. Press Ctrl+C to attempt graceful shutdown.")
    except Exception as e_signal_setup: # ä¾‹å¦‚åœ¨éä¸»çº¿ç¨‹ä¸­è®¾ç½® (è™½ç„¶è¿™é‡Œæ˜¯ä¸»çº¿ç¨‹)
        g_main_logger.error(f"Failed to set signal handlers: {e_signal_setup}", exc_info=True)

    # --- æ ¸å¿ƒè®­ç»ƒæµç¨‹ ---
    try:
        g_main_logger.info("="*60)
        g_main_logger.info(f"Starting Experiment: {CONFIG['experiment_meta']['experiment_name']} (Main Process PID: {os.getpid()})")
        g_main_logger.info(f"Log paths: {main_log_paths}")
        g_main_logger.info("="*60)

        # 1. åˆ›å»º InferenceServer é€šä¿¡é˜Ÿåˆ—
        g_learner_to_server_cmd_q = mp.Queue()   # Learner -> Server çš„å‘½ä»¤é˜Ÿåˆ—
        actors_to_server_req_q = mp.Queue()      # Actors -> Server çš„æ¨ç†è¯·æ±‚é˜Ÿåˆ—
        
        # Server -> Actors çš„å“åº”é˜Ÿåˆ— (æ¯ä¸ª Actor ä¸€ä¸ª)
        server_to_actors_resp_qs = {}
        for i in range(CONFIG['num_actors']):
            actor_id_key = f'Actor-{CONFIG.get("actor_id_base", 0) + i}' # ä¸ Actor é…ç½®ä¸­çš„ name ä¿æŒä¸€è‡´
            server_to_actors_resp_qs[actor_id_key] = mp.Queue()
        g_main_logger.info("Communication queues for InferenceServer created.")        # 2. å‡†å¤‡ InferenceServer é…ç½®
        
        benchmark_models_info_for_server = {}
        # ä» benchmark_policies é…ç½®ä¸­è·å–åŸºå‡†æ¨¡å‹ä¿¡æ¯
        for policy_name, policy_path in CONFIG.get('benchmark_policies', {}).items():
            if policy_path and os.path.isfile(policy_path):
                benchmark_models_info_for_server[policy_name] = {"path": policy_path}
                g_main_logger.info(f"Added benchmark model '{policy_name}' from path: {policy_path}")
            else:
                g_main_logger.warning(f"Benchmark model path not found: {policy_path} for policy '{policy_name}'")
        
        server_config = {
            'name': 'InferenceServer', # æœåŠ¡å™¨è¿›ç¨‹çš„åç§°
            'server_id': f"inference_server_{os.getpid()}", # ç»™æœåŠ¡å™¨ä¸€ä¸ªå”¯ä¸€IDï¼Œç”¨äºæ—¥å¿—

            'benchmark_models_info': benchmark_models_info_for_server, # è¦æ‰˜ç®¡çš„åŸºå‡†æ¨¡å‹
        }
        server_config.update(CONFIG['experiment_meta']) # æ·»åŠ å®éªŒå…ƒæ•°æ®
        server_config.update(CONFIG['model_agent_config']) # æ·»åŠ æ¨¡å‹å’Œä»£ç†é…ç½®
        server_config.update(CONFIG['inference_server_config']) # æ·»åŠ  InferenceServer ç‰¹å®šé…ç½®

        g_main_logger.info("InferenceServer configuration prepared.")

        # 3. å®ä¾‹åŒ–å¹¶å¯åŠ¨ InferenceServer
        g_inference_server_process = InferenceServer(
            server_config,
            g_learner_to_server_cmd_q,
            actors_to_server_req_q,
            server_to_actors_resp_qs
        )
        g_main_logger.info("Starting InferenceServer process...")
        g_inference_server_process.start()
        time.sleep(3) # ç»™æœåŠ¡å™¨ä¸€ç‚¹æ—¶é—´åˆå§‹åŒ– (å¯é€‰ï¼Œä½†æœ‰æ—¶æœ‰å¸®åŠ©)
        if not g_inference_server_process.is_alive():
            g_main_logger.critical("InferenceServer failed to start! Check its logs. Exiting.")
            cleanup_all_processes() # å°è¯•æ¸…ç†
            return # ä¸»ç¨‹åºé€€å‡º
        g_main_logger.info(f"InferenceServer process (PID: {g_inference_server_process.pid}) started.")


        # 4. åˆå§‹åŒ– Replay Buffer
        g_main_logger.info("Initializing Replay Buffer...")
        replay_buffer = ReplayBuffer(
            CONFIG['replay_buffer_size'], 
            CONFIG['replay_buffer_episode_capacity'],
        )

        g_main_logger.info("Replay Buffer initialized.")

        # 5. å‡†å¤‡å¹¶å¯åŠ¨ Learner
        checkpoint_dir = main_log_paths['checkpoint_dir']  # ä½¿ç”¨æ–°çš„è·¯å¾„
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        learner_config = CONFIG['learner_config'].copy() # ä¸º Learner åˆ›å»ºé…ç½®å‰¯æœ¬
        learner_config.update(CONFIG['experiment_meta']) # æ·»åŠ å®éªŒå…ƒæ•°æ®
        learner_config.update(CONFIG['model_agent_config']) # æ·»åŠ æ¨¡å‹å’Œä»£ç†é…ç½®
        learner_config.update(CONFIG['ppo_config']) # æ·»åŠ  PPO é…ç½®

        learner_config['ckpt_save_path'] = checkpoint_dir # ä¼ é€’æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„
        learner_config['shutdown_event'] = shutdown_event # ä¼ é€’å…³é—­äº‹ä»¶
        learner_config['inference_server_cmd_queue'] = g_learner_to_server_cmd_q # ä¼ é€’å‘½ä»¤é˜Ÿåˆ—
        learner_config['log_base_dir'] = log_base_dir


        g_main_logger.info("Initializing Learner...")
        g_learner_process = Learner(learner_config, replay_buffer)
        g_main_logger.info("Learner initialized.")
        g_main_logger.info("Starting Learner process...")
        g_learner_process.start()
        time.sleep(1) # ç»™ Learner ä¸€ç‚¹æ—¶é—´å¯åŠ¨
        if not g_learner_process.is_alive():
            g_main_logger.critical("Learner failed to start! Check its logs. Exiting.")
            cleanup_all_processes()
            return
        g_main_logger.info(f"Learner process (PID: {g_learner_process.pid}) started.")


        # 6. å‡†å¤‡å¹¶å¯åŠ¨ Actors
        g_main_logger.info(f"Initializing {CONFIG['num_actors']} Actors...")
        g_actor_processes.clear() # æ¸…ç©ºå…¨å±€åˆ—è¡¨ä»¥é˜²é‡ç”¨
        for i in range(CONFIG['num_actors']):

            actor_config = CONFIG['actor_config'].copy() # è·å– Actor ç‰¹å®šé…ç½®
            actor_config.update(CONFIG['experiment_meta']) # æ·»åŠ å®éªŒå…ƒæ•°æ®
            actor_config.update(CONFIG['model_agent_config']) # æ·»åŠ æ¨¡å‹å’Œä»£ç†é…ç½®
            actor_config.update(CONFIG['ppo_config']) # æ·»åŠ  PPO é…ç½®
            actor_config.update(CONFIG['profiling_config']) # æ·»åŠ  profiling é…ç½®

            actor_config['server_hosted_benchmark_names'] = CONFIG['server_hosted_benchmark_names'] # æ·»åŠ æœåŠ¡å™¨æ‰˜ç®¡çš„åŸºå‡†æ¨¡å‹åç§°

            actor_id_val = i # è®¡ç®—å”¯ä¸€çš„ Actor ID
            actor_name_key = f'Actor-{actor_id_val}' # åˆ›å»º Actor åç§°
            
            actor_config['name'] = actor_name_key
            actor_config['actor_id'] = actor_id_val # å°† actor_id ä¼ å…¥é…ç½®ï¼Œä¾› Actor å†…éƒ¨ä½¿ç”¨
            actor_config['shutdown_event'] = shutdown_event # ä¼ é€’å…³é—­äº‹ä»¶
            actor_config['inference_server_req_queue'] = actors_to_server_req_q # æ¨ç†è¯·æ±‚é˜Ÿåˆ—
            actor_config['inference_server_resp_queue'] = server_to_actors_resp_qs[actor_name_key] # ä¸“å±çš„å“åº”é˜Ÿåˆ—
            actor_config['log_base_dir'] = log_base_dir # æ—¥å¿—ç›®å½•
                        # ç®€åŒ–ï¼šåªä¸ºActor-0åœ¨ä¸»TensorBoardä¸­è®°å½•æŒ‡æ ‡ï¼Œé¿å…å¤šè¿›ç¨‹å†™å…¥å†²çª
            # å…¶ä»–Actoråªå†™å…¥è‡ªå·±çš„detailed TensorBoard
            if actor_id_val == 0:  # åªæœ‰Actor-0å†™å…¥ä¸»TensorBoard
                main_tensorboard_dir = os.path.join(main_log_paths['main_experiment_dir'], 'tensorboard', 'main_train')
                actor_main_tb_path = os.path.join(main_tensorboard_dir, 'Actor-0')
                actor_config['actor_main_tensorboard_path'] = actor_main_tb_path
            else:
                # å…¶ä»–Actorä¸å†™å…¥ä¸»TensorBoard
                actor_config['actor_main_tensorboard_path'] = None

            actor = Actor(actor_config, replay_buffer)
            g_actor_processes.append(actor)
        g_main_logger.info(f"{CONFIG['num_actors']} Actors initialized.")
        g_main_logger.info("Starting Actor processes...")
        for actor in g_actor_processes:
            actor.start()
        g_main_logger.info("All Actor processes started.")

        # --- 7. ç­‰å¾…è¿›ç¨‹ç»“æŸ (ä¸»è®­ç»ƒå¾ªç¯å¯¹äº train.py æ¥è¯´å°±æ˜¯ç­‰å¾…) ---
        g_main_logger.info("Main process is now waiting for Actor processes to complete their configured episodes (or until a shutdown signal is received)...")
        
        # è®°å½•è®­ç»ƒå¼€å§‹çš„é‡è¦ä¿¡æ¯
        total_env_steps = CONFIG['actor_config']['num_env_steps'] 
        actors_count = CONFIG['num_actors']
        envs_per_actor = CONFIG['actor_config']['num_envs_per_actor']
        total_parallel_envs = actors_count * envs_per_actor
        
        g_main_logger.info(f"ğŸš€ TRAINING_START | Target: {total_env_steps:,} steps | "
                          f"Parallel Envs: {total_parallel_envs} ({actors_count} actors Ã— {envs_per_actor} envs) | "
                          f"Expected Duration: ~{total_env_steps / (total_parallel_envs * 60):.1f} hours")
        
        # ç›‘æ§å˜é‡
        training_start_time = time.time()  # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        last_status_time = time.time()
        status_interval = 60  # æ¯60ç§’è¾“å‡ºä¸€æ¬¡çŠ¶æ€
        
        for actor in g_actor_processes:
            while actor.is_alive(): # åªè¦ actor è¿˜åœ¨è¿è¡Œ
                if shutdown_event.is_set(): # æ£€æŸ¥æ˜¯å¦æ”¶åˆ°äº†å…³é—­ä¿¡å·
                    g_main_logger.info(f"Main process detected shutdown_event, no longer actively waiting for Actor {actor.name}.")
                    break # è·³å‡ºå¯¹æ­¤ actor çš„ç­‰å¾…
                
                # å®šæœŸè¾“å‡ºè®­ç»ƒçŠ¶æ€
                current_time = time.time()
                if current_time - last_status_time >= status_interval:
                    try:
                        # è·å–ReplayBufferçŠ¶æ€
                        buffer_size = replay_buffer.size()
                        buffer_episodes = replay_buffer.queue.qsize() if hasattr(replay_buffer, 'queue') and hasattr(replay_buffer.queue, 'qsize') else 'N/A'
                        
                        # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
                        learner_status = "RUNNING" if g_learner_process and g_learner_process.is_alive() else "STOPPED"
                        server_status = "RUNNING" if g_inference_server_process and g_inference_server_process.is_alive() else "STOPPED"
                        alive_actors = sum(1 for a in g_actor_processes if a.is_alive())
                        
                        elapsed_minutes = (current_time - training_start_time) / 60
                        estimated_progress = min(buffer_size / total_env_steps * 100, 100) if total_env_steps > 0 else 0
                        
                        g_main_logger.info(f"ğŸ“Š TRAINING_STATUS | ReplayBuffer: {buffer_size:,} steps ({buffer_episodes} episodes) | "
                                         f"Processes: Learner={learner_status}, Server={server_status}, Actors={alive_actors}/{len(g_actor_processes)} | "
                                         f"Runtime: {elapsed_minutes:.1f}min | Progress: {estimated_progress:.1f}%")
                        
                        # è®°å½•ä¸»è¦çŠ¶æ€åˆ°TensorBoard
                        if g_main_writer:
                            try:
                                g_main_writer.add_scalar('System/ReplayBuffer_Size', buffer_size, int(elapsed_minutes * 60))
                                g_main_writer.add_scalar('System/Training_Progress_Percent', estimated_progress, int(elapsed_minutes * 60))
                                g_main_writer.add_scalar('System/Active_Actors', alive_actors, int(elapsed_minutes * 60))
                                g_main_writer.flush()
                            except Exception as e_main_tb:
                                g_main_logger.warning(f"Failed to write system metrics to main TensorBoard: {e_main_tb}")
                        
                        last_status_time = current_time
                    except Exception as e_status:
                        g_main_logger.warning(f"Error collecting training status: {e_status}")
                
                actor.join(timeout=1.0) # å¸¦è¶…æ—¶çš„ joinï¼Œå…è®¸ä¸»è¿›ç¨‹å‘¨æœŸæ€§åœ°æ£€æŸ¥ shutdown_event
            if not shutdown_event.is_set() and not actor.is_alive(): # å¦‚æœ actor æ­£å¸¸ç»“æŸ
                 g_main_logger.info(f"âœ… ACTOR_COMPLETED | {actor.name} (PID: {actor.pid if actor.pid else 'N/A'}) has finished its episodes.")
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        total_training_time = (time.time() - training_start_time) / 60
        final_buffer_size = replay_buffer.size()
        
        if not shutdown_event.is_set(): # å¦‚æœä¸æ˜¯å› ä¸ºå¤–éƒ¨ä¿¡å·ä¸­æ–­çš„ (å³æ‰€æœ‰ Actors éƒ½æ­£å¸¸å®Œæˆäº†)
            g_main_logger.info(f"ğŸ¯ TRAINING_COMPLETED | All Actor processes have completed their tasks. | "
                              f"Total Runtime: {total_training_time:.1f}min | "
                              f"Final Buffer Size: {final_buffer_size:,} steps")
            
            # è®°å½•è®­ç»ƒå®ŒæˆæŒ‡æ ‡åˆ°TensorBoard
            if g_main_writer:
                try:
                    g_main_writer.add_scalar('Training/Total_Runtime_Minutes', total_training_time, final_buffer_size)
                    g_main_writer.add_scalar('Training/Final_Buffer_Size', final_buffer_size, final_buffer_size)
                    g_main_writer.add_scalar('Training/Completion_Status', 1.0, final_buffer_size)  # 1.0 è¡¨ç¤ºæ­£å¸¸å®Œæˆ
                    g_main_writer.flush()
                except Exception as e_final_tb:
                    g_main_logger.warning(f"Failed to write completion metrics to main TensorBoard: {e_final_tb}")
                    
            g_main_logger.info("Signaling Learner and InferenceServer to shut down gracefully...")
            shutdown_event.set() # é€šçŸ¥ Learner ä¼˜é›…å…³é—­
            if g_learner_to_server_cmd_q: # é€šçŸ¥ InferenceServer ä¼˜é›…å…³é—­
                try:
                    g_learner_to_server_cmd_q.put(("SHUTDOWN", None), timeout=5)
                except Exception as e_shutdown_cmd:
                     g_main_logger.error(f"Error sending SHUTDOWN command to InferenceServer: {e_shutdown_cmd}")
        
        # æœ€ç»ˆçš„ç­‰å¾…å’Œæ¸…ç†ç”± cleanup_all_processes å‡½æ•°å¤„ç†

    except KeyboardInterrupt:
        g_main_logger.warning("Main process caught KeyboardInterrupt (Ctrl+C). Cleanup will be called by signal handler or finally block.")
        # ä¿¡å·å¤„ç†å™¨ cleanup_all_processes åº”è¯¥å·²ç»è¢«è§¦å‘äº†
        # å¦‚æœç”±äºæŸç§åŸå› æ²¡æœ‰ï¼ˆä¾‹å¦‚ï¼Œä¿¡å·å¤„ç†å™¨è®¾ç½®å¤±è´¥ï¼‰ï¼Œfinally å—ä¼šä½œä¸ºåå¤‡
    except Exception as e:
        g_main_logger.critical(f"Main training orchestration loop encountered an unhandled exception: {e}", exc_info=True)
        # åœ¨å‘ç”Ÿå…¶ä»–ä¸¥é‡é”™è¯¯æ—¶ï¼Œä¹Ÿå°è¯•æ¸…ç†
    finally:
        g_main_logger.info("Main process entering 'finally' block for cleanup.")
        # ç¡®ä¿ cleanup_all_processes è‡³å°‘è¢«è°ƒç”¨ä¸€æ¬¡ï¼Œä»¥å¤„ç†æ‰€æœ‰å­è¿›ç¨‹å’Œèµ„æº
        if not (hasattr(cleanup_all_processes, 'is_shutting_down') and cleanup_all_processes.is_shutting_down):
            cleanup_all_processes() # å¦‚æœä¿¡å·å¤„ç†å™¨æœªè¿è¡Œæˆ–æå‰å¤±è´¥ï¼Œè¿™é‡Œä¼šè°ƒç”¨
        
        # æ¢å¤åŸå§‹ä¿¡å·å¤„ç†å™¨ (é€šå¸¸åœ¨ç¨‹åºçœŸæ­£é€€å‡ºå‰æ‰§è¡Œï¼Œä»¥é˜²å½±å“åç»­å¯èƒ½çš„Pythonä»£ç )
        try:
            if 'original_sigint_handler' in locals() and original_sigint_handler is not None: # æ£€æŸ¥å˜é‡æ˜¯å¦å·²å®šä¹‰
                 signal.signal(signal.SIGINT, original_sigint_handler)
            if 'original_sigterm_handler' in locals() and original_sigterm_handler is not None:
                 signal.signal(signal.SIGTERM, original_sigterm_handler)
        except Exception as e_restore_final:
            # ä½¿ç”¨ log_func ä»¥é˜² g_main_logger æ­¤æ—¶ä¹Ÿå¯èƒ½å‡ºé—®é¢˜
            log_func_final = g_main_logger.warning if g_main_logger else print
            log_func_final(f"Finally block: Could not restore original signal handlers: {e_restore_final}")

        log_func_final = g_main_logger.info if g_main_logger else print
        log_func_final("Training script main function finished.")

if __name__ == '__main__':
    try:
        # å°è¯•è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ä¸º 'spawn'ï¼Œè¿™åœ¨æŸäº›ç³»ç»Ÿï¼ˆå¦‚macOS, Windowsï¼‰å’Œä½¿ç”¨CUDAæ—¶æ›´ç¨³å®š
        # éœ€è¦åœ¨ä»»ä½•å¤šè¿›ç¨‹å¯¹è±¡åˆ›å»ºä¹‹å‰è°ƒç”¨
        current_start_method = mp.get_start_method(allow_none=True)
        if current_start_method != 'spawn': # åªæœ‰å½“ä¸æ˜¯ 'spawn' æ—¶æ‰å°è¯•è®¾ç½®
            mp.set_start_method('spawn', force=True) 
            print(f"å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•å·²è®¾ç½®ä¸º 'spawn'ã€‚ä¹‹å‰ä¸º: {current_start_method}")
        else:
            print(f"å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•å·²ç»æ˜¯ 'spawn'ã€‚")
    except RuntimeError as e_sm_runtime:
        # ä¾‹å¦‚ï¼Œå¦‚æœä¸Šä¸‹æ–‡å·²ç»è®¾ç½®ï¼Œæˆ–è€…åœ¨æŸäº›ç¯å¢ƒä¸­ä¸å…è®¸æ›´æ”¹
        print(f"ä¿¡æ¯: å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•æœªèƒ½è®¾ç½®ä¸º 'spawn' (ä¾‹å¦‚ï¼Œå·²å¯åŠ¨æˆ–å¹³å°ä¸æ”¯æŒæ›´æ”¹): {e_sm_runtime}")
    except Exception as e_sm_main:
        print(f"è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•æ—¶å‘ç”Ÿé”™è¯¯: {e_sm_main}")
        
    main()
