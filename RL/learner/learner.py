# å¯¼å…¥å¤šè¿›ç¨‹æ¨¡å—ï¼ŒLearner å°†ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„è¿›ç¨‹è¿è¡Œ
from multiprocessing import Process
import time
import numpy as np
import torch
from torch.nn import functional as F # å¯¼å…¥ PyTorch å‡½æ•°åº“ï¼Œé€šå¸¸ç”¨äºæŸå¤±å‡½æ•°ã€æ¿€æ´»å‡½æ•°ç­‰
import os
# import logging # logging is handled by setup_process_logging_and_tensorboard
from torch.utils.tensorboard.writer import SummaryWriter # å¼•å…¥ TensorBoard
import json # ç”¨äºæ‰“å°é…ç½®

from utils import calculate_scheduled_lr # å¯¼å…¥åŠ¨æ€å­¦ä¹ ç‡è®¡ç®—å‡½æ•°
from utils import setup_process_logging_and_tensorboard # å¯¼å…¥æ—¥å¿—å’Œ TensorBoard è®¾ç½®å‡½æ•°

import signal # ç”¨äºå¼‚å¸¸å¤„ç†æ—¶ gracefully exit
import sys # ç”¨äº sys.exit

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from replay_buffer import ReplayBuffer      # ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œç”¨äºå­˜å‚¨ Actor æ”¶é›†çš„æ•°æ®å¹¶ä¾› Learner é‡‡æ ·
# from model_pool_extended import ModelPoolServer      # æ¨¡å‹æ± æœåŠ¡å™¨ï¼Œç”¨äºç®¡ç†å’Œåˆ†å‘æ¨¡å‹ç‰ˆæœ¬
# å‡è®¾æ‚¨çš„æ¨¡å‹å®šä¹‰åœ¨ model.py ä¸­
# from model import ResNet34AC, ResNet34 # , ResNetFeatureExtractor # ç¡®ä¿å¯¼å…¥äº†éœ€è¦çš„æ¨¡å‹ç±»
from models.actor import ResNet34Actor
from models.critic import ResNet34CentralizedCritic
from collections import OrderedDict # ç”¨äºæœ‰åºå­—å…¸ï¼Œä¿æŒå‚æ•°åŠ è½½é¡ºåº
from algos.ppo import PPOAlgorithm # å¯¼å…¥ PPO ç®—æ³•å®ç°

# Learner ç±»ï¼Œç»§æ‰¿è‡ª Processï¼Œè´Ÿè´£æ¨¡å‹çš„è®­ç»ƒå’Œæ›´æ–°
class Learner(Process):

    # åˆå§‹åŒ–å‡½æ•°
    def __init__(self, config, replay_buffer):
        """
        åˆå§‹åŒ– Learner è¿›ç¨‹ã€‚

        Args:
            config (dict): åŒ…å« Learner é…ç½®å‚æ•°çš„å­—å…¸ (ä¾‹å¦‚æ¨¡å‹æ± å¤§å°/åç§°, è®¾å¤‡, å­¦ä¹ ç‡, PPOè¶…å‚æ•°ç­‰)ã€‚
            replay_buffer (ReplayBuffer): Learner ä»ä¸­é‡‡æ ·æ•°æ®è¿›è¡Œè®­ç»ƒçš„å…±äº«ç»éªŒå›æ”¾ç¼“å†²åŒºã€‚
        """
        super(Learner, self).__init__() # è°ƒç”¨çˆ¶ç±» Process çš„åˆå§‹åŒ–æ–¹æ³•
        self.replay_buffer = replay_buffer # å­˜å‚¨ä¼ å…¥çš„ replay_buffer å®ä¾‹
        self.config = config                # å­˜å‚¨é…ç½®å­—å…¸
        # Learner é€šå¸¸æ˜¯å•ä¾‹ï¼Œä¸éœ€è¦åƒ Actor é‚£æ ·é€šè¿‡ config ä¼ é€’ nameï¼Œå¯ç›´æ¥å‘½å
        self.name = "Learner"
        self.logger = None # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ä¸º None
        self.writer = None # åˆå§‹åŒ– TensorBoard writer ä¸º None
        self.model = None # Initialize model attribute

        self.inference_server_cmd_queue = config.get('inference_server_cmd_queue')
        if self.inference_server_cmd_queue is None:
            # å¦‚æœæ²¡æœ‰æä¾›å‘½ä»¤é˜Ÿåˆ—ï¼ŒLearner å°†æ— æ³•ä¸ InferenceServer é€šä¿¡
            # æ ¹æ®æ‚¨çš„è®¾è®¡ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªè‡´å‘½é”™è¯¯
            # è¿™é‡Œå¯ä»¥å…ˆè®°å½•ä¸€ä¸ªè­¦å‘Šï¼Œåœ¨ run æ–¹æ³•å¼€å§‹æ—¶å†åšæ›´ä¸¥æ ¼çš„æ£€æŸ¥
            print(f"Warning: Learner æœªæ”¶åˆ° 'inference_server_cmd_queue'ã€‚å°†æ— æ³•æ›´æ–° InferenceServerã€‚")
            # (åœ¨å®é™…ä»£ç ä¸­ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨è¿™é‡Œå°±æŠ›å‡ºå¼‚å¸¸æˆ–é€€å‡º)

        self.shutdown_event = config.get('shutdown_event')
        if self.shutdown_event is None:
            print(f"Warning: Learner æœªæ”¶åˆ° 'shutdown_event'ã€‚å¯èƒ½æ— æ³•ä¼˜é›…å…³é—­ã€‚")

        self.shutting_down = False # ä¿¡å·å¤„ç†æˆ–å…³é—­äº‹ä»¶çš„æ ‡å¿—
        
    def _signal_handler(self, signum, frame):
        """å¤„ç†ç»ˆæ­¢ä¿¡å·ï¼Œç¡®ä¿èµ„æºè¢«æ¸…ç†ã€‚"""
        if self.shutting_down: # é˜²æ­¢é‡å…¥
            return
        self.shutting_down = True

        signal_name = signal.Signals(signum).name
        self.logger.warning(f"Learner process {self.name} (PID: {os.getpid()}) received signal {signal_name} ({signum}). Initiating graceful shutdown...")
        
        # å…³é—­ TensorBoard writer (å¦‚æœå­˜åœ¨)
        if self.writer:
            self.logger.info("Learner closing TensorBoard writer...")
            try:
                self.writer.close()
            except Exception as e_writer:
                self.logger.error(f"Error closing TensorBoard writer in signal handler: {e_writer}", exc_info=True)
        
        # æ¸…ç†å¯èƒ½æ³„éœ²çš„å…±äº«å†…å­˜å’ŒCUDAèµ„æº
        self.logger.info("Cleaning up shared memory and CUDA resources...")
        self._cleanup_resources()

        self.logger.info(f"Learner {self.name} shutdown tasks complete. Exiting via signal handler.")
        # åœ¨ä¿¡å·å¤„ç†å™¨ä¸­ï¼Œé€šå¸¸å»ºè®®é‡æ–°æŠ›å‡ºä¿¡å·æˆ–ä»¥ç›¸åº”çš„é€€å‡ºç é€€å‡º
        # sys.exit(128 + signum) # å¸¸è§çš„é€€å‡ºç çº¦å®š
        # æˆ–è€…ï¼Œå¦‚æœå¸Œæœ›Pythonçš„é»˜è®¤ä¿¡å·å¤„ç†ï¼ˆå¦‚æ‰“å°KeyboardInterruptï¼‰å‘ç”Ÿï¼š
        # signal.signal(signum, signal.SIG_DFL) # æ¢å¤é»˜è®¤å¤„ç†å™¨
        # os.kill(os.getpid(), signum) # é‡æ–°å‘é€ä¿¡å·ç»™è‡ªå·±
        # ç®€å•èµ·è§ï¼Œå¯ä»¥ç›´æ¥é€€å‡ºï¼Œä½†å¯èƒ½ä¸ä¼šæ‰“å°æ ‡å‡† KeyboardInterrupt æ¶ˆ Messages
        sys.exit(0) # æˆ–è€…ä¸€ä¸ªè¡¨ç¤ºå¼‚å¸¸ç»ˆæ­¢çš„éé›¶é€€å‡ºç 
        
    def _set_requires_grad(self, module, requires_grad):
        """Helper function to set requires_grad for all parameters of a module."""
        if module is None:
            self.logger.warning(f"Attempted to set requires_grad on a None module.")
            return
            
        for param in module.parameters():
            param.requires_grad = requires_grad
        
        # åˆ¤æ–­æ¨¡å—æ˜¯å±äºactorè¿˜æ˜¯critic
        component_name = "Unknown"
        if hasattr(self, 'actor'):
            # æ£€æŸ¥æ˜¯å¦æ˜¯actorçš„ä¸€éƒ¨åˆ†
            for name, mod in self.actor.named_children():
                if mod is module:
                    component_name = f"Actor.{name}"
                    break
        
        if component_name == "Unknown" and hasattr(self, 'critic'):
            # æ£€æŸ¥æ˜¯å¦æ˜¯criticçš„ä¸€éƒ¨åˆ†
            for name, mod in self.critic.named_children():
                if mod is module:
                    component_name = f"Critic.{name}"
                    break
        
        status = "TRAINABLE" if requires_grad else "FROZEN"
        self.logger.info(f"Component '{component_name}' is now {status}.")


    # è¿›ç¨‹å¯åŠ¨æ—¶æ‰§è¡Œçš„ä¸»å‡½æ•°
    def run(self):
        # --- è®¾ç½®ä¿¡å·å¤„ç†å™¨ ---
        # é€šå¸¸åœ¨è¿›ç¨‹çš„ä¸»è¦æ‰§è¡Œé€»è¾‘å¼€å§‹æ—¶è®¾ç½®
        # æ³¨æ„ï¼šåœ¨ Windows ä¸Šï¼Œsignal.SIGINT å¯èƒ½åªèƒ½è¢«ä¸»çº¿ç¨‹æ•è·ã€‚
        # å¯¹äºå¤šè¿›ç¨‹ï¼Œæ›´é€šç”¨çš„æ–¹å¼å¯èƒ½æ˜¯åœ¨ä¸»è¿›ç¨‹ä¸­æ•è·ï¼Œç„¶åé€šçŸ¥å­è¿›ç¨‹ã€‚
        # ä½†å¦‚æœ Learner æ˜¯ä¸»å¯¼ ModelPoolServer çš„è¿›ç¨‹ï¼Œå®ƒè‡ªèº«å¤„ç†ä¿¡å·æ˜¯åˆç†çš„ã€‚
        original_sigint_handler = signal.getsignal(signal.SIGINT)
        original_sigterm_handler = signal.getsignal(signal.SIGTERM)
        try:
            signal.signal(signal.SIGINT, self._signal_handler)  # Ctrl+C
            signal.signal(signal.SIGTERM, self._signal_handler) # kill å‘½ä»¤
        except ValueError as e_signal: # ä¾‹å¦‚åœ¨éä¸»çº¿ç¨‹ä¸­è®¾ç½® SIGINT å¯èƒ½ä¼šå¤±è´¥
            print(f"Warning: Could not set signal handlers for Learner: {e_signal}")
            # self.logger å¯èƒ½è¿˜æœªåˆå§‹åŒ–ï¼Œæ‰€ä»¥ç”¨ print


        # --- 1. åˆå§‹åŒ–æ—¥å¿—å’Œ TensorBoard ---
        # æ ¹æ®é…ç½®å†³å®šä½¿ç”¨ä¸»è¦æ—¥å¿—è¿˜æ˜¯è¯¦ç»†æ—¥å¿—
        log_type = 'main'  # learnerä½¿ç”¨ä¸»è¦æ—¥å¿—ï¼Œè®°å½•è®­ç»ƒä¿¡æ¯
        
        self.logger, self.writer, learner_log_paths = setup_process_logging_and_tensorboard(
            self.config['log_base_dir'], self.config, self.name, log_type=log_type
        )
        
        # ä¸ºè¯¦ç»†æŒ‡æ ‡åˆ›å»ºadditional writerï¼ˆç”¨äºReplayBufferå’ŒPerformanceæŒ‡æ ‡ï¼‰
        try:
            self.detailed_logger, self.detailed_writer, _ = setup_process_logging_and_tensorboard(
                self.config['log_base_dir'], self.config, self.name, log_type='detailed'
            )
        except Exception as e:
            self.logger.warning(f"Failed to setup detailed logging for Learner: {e}. ReplayBuffer and Performance metrics will not be logged.")
            self.detailed_writer = None
        
        if not self.logger:
            print(f"CRITICAL: Logger for {self.name} could not be initialized. Exiting.")
            if self.writer: self.writer.close()
            if hasattr(self, 'detailed_writer') and self.detailed_writer: self.detailed_writer.close()
            return
        self.logger.info(f"Learner process {self.name} started. PID: {os.getpid()}.")
        
        # æ£€æŸ¥TensorBoard writerçŠ¶æ€
        if self.writer:
            self.logger.info(f"âœ… Main TensorBoard writeråˆ›å»ºæˆåŠŸ: {learner_log_paths.get('tensorboard_path', 'Unknown path')}")
        else:
            self.logger.warning(f"âŒ Main TensorBoard writeråˆ›å»ºå¤±è´¥!")
            
        if hasattr(self, 'detailed_writer') and self.detailed_writer:
            self.logger.info(f"âœ… Detailed TensorBoard writeråˆ›å»ºæˆåŠŸ")
        else:
            self.logger.warning(f"âŒ Detailed TensorBoard writeråˆ›å»ºå¤±è´¥æˆ–ä¸å­˜åœ¨")
            
        # ç«‹å³å†™å…¥ä¸€ä¸ªæµ‹è¯•æŒ‡æ ‡ä»¥éªŒè¯TensorBoardå·¥ä½œ
        if self.writer:
            try:
                self.writer.add_scalar('Test/Learner_Initialization', 1.0, 0)
                self.writer.flush()
                self.logger.info("âœ… æˆåŠŸå†™å…¥æµ‹è¯•TensorBoardæŒ‡æ ‡")
            except Exception as e:
                self.logger.error(f"âŒ æµ‹è¯•TensorBoardå†™å…¥å¤±è´¥: {e}")

        # --- ä¿®æ”¹éƒ¨åˆ†ï¼šæ›´å®‰å…¨åœ°è®°å½•é…ç½®ä¿¡æ¯ ---
        config_to_log = {}
        known_unserializable_keys = ['shutdown_event', 'inference_server_cmd_queue'] # ä»¥åŠå…¶ä»–å¯èƒ½çš„é˜Ÿåˆ—æˆ–äº‹ä»¶å¯¹è±¡

        for key, value in self.config.items():
            if key in known_unserializable_keys:
                config_to_log[key] = f"<{type(value).__name__} object at {hex(id(value))}>" # æˆ–è€… str(value)
            elif callable(value) and hasattr(value, '__name__'): # ä¾‹å¦‚æ¨¡å‹ç±»
                config_to_log[key] = f"<class '{value.__name__}'>"
            else:
                # å¯¹äºå…¶ä»–å€¼ï¼Œå…ˆå°è¯•ç›´æ¥å¤åˆ¶ï¼Œjson.dumps çš„ default ä¼šå¤„ç†åç»­é—®é¢˜
                config_to_log[key] = value 
        
        try:
            # ä½¿ç”¨ default=str ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆå¤„ç†å…¶ä»–æœªé¢„æ–™åˆ°çš„ä¸å¯åºåˆ—åŒ–ç±»å‹
            config_json_str = json.dumps(config_to_log, indent=2, ensure_ascii=False, default=str)
            self.logger.info(f"Learner Config (serializable view): \n{config_json_str}")
        except Exception as e_json:
            self.logger.error(f"Failed to serialize config for logging: {e_json}. Logging config keys only.")
            # å¦‚æœä¸Šé¢ä»ç„¶å¤±è´¥ï¼Œåªè®°å½•é”®åæˆ–æ›´ç®€å•çš„è¡¨ç¤º
            config_keys_str = json.dumps(list(self.config.keys()), indent=2, ensure_ascii=False)
            self.logger.info(f"Learner Config Keys: \n{config_keys_str}")

        # self.logger.info(f"Learner process {self.name} started. Config: \n{json.dumps(self.config, indent=2, ensure_ascii=False)}")

        # æ£€æŸ¥å¿…è¦çš„é˜Ÿåˆ—æ˜¯å¦å­˜åœ¨
        if self.inference_server_cmd_queue is None:
            self.logger.critical("'inference_server_cmd_queue' not provided in config. Learner cannot communicate with InferenceServer. Exiting.")
            # ... (æ¢å¤ä¿¡å·å¤„ç†å™¨å¹¶é€€å‡º) ...
            return
        if self.shutdown_event is None:
            self.logger.warning("'shutdown_event' not provided. Learner might not shut down gracefully via main process signal.")
            # å¯ä»¥é€‰æ‹©ä¸é€€å‡ºï¼Œä½†è¿™æ˜¯ä¸€ä¸ªæ½œåœ¨é—®é¢˜
        
        self.logger.info("Learner will communicate with an external InferenceServer.")

        # åç»­é€»è¾‘å…¨éƒ¨æ”¾åœ¨ try ä¸­, ä½¿å¾—æˆ‘ä»¬æ‰‹åŠ¨ç»ˆæ­¢ç¨‹åºæ—¶ä¼šè‡ªåŠ¨æ¸…ç† ModelPoolServer å®ä¾‹
        try:
            # --- 1. åˆ›å»ºæ¨¡å‹å’Œç®—æ³•å®ä¾‹ ---
            device = torch.device(self.config['device'])
            self.logger.info(f"Learner will train model on device: {device}")
            
            # a. åˆ›å»ºæ¨¡å‹å®ä¾‹ (Actor å’Œ Critic)
            #    è¿™é‡Œå‡è®¾æ‚¨æœ‰ä¸€ä¸ªä¸»æ¨¡å‹ç±»ï¼Œå®ƒå†…éƒ¨åŒ…å«äº† actor å’Œ critic
            #    å¦‚æœå®ƒä»¬æ˜¯å®Œå…¨åˆ†ç¦»çš„ï¼Œåˆ™åˆ†åˆ«åˆ›å»º
            self.actor = self.config['actor_class'](
                in_channels=self.config['in_channels'],
                out_channels=self.config['out_channels'],
            ).to(device)
            self.critic = self.config['critic_class'](
                in_channels_obs=self.config['in_channels'],
                in_channels_extra=self.config.get('critic_extra_in_channels', 0),
            ).to(device)

            self.logger.info("RL model instance created.")

            # b. ä» SL æ¨¡å‹åŠ è½½åˆå§‹æƒé‡
        
            # ä»é…ç½®ä¸­è·å–åˆå§‹ Actor æ¨¡å‹è·¯å¾„
            initial_actor_path = self.config.get('initial_actor_eval_path')
            state_dict = torch.load(initial_actor_path, map_location=device)
            self.actor.load_state_dict(state_dict)
        
            # ä»é…ç½®ä¸­è·å–åˆå§‹ Critic æ¨¡å‹è·¯å¾„
            initial_critic_path = self.config.get('initial_critic_eval_path')
            critic_state_dict = torch.load(initial_critic_path, map_location=device)
            self.critic.load_state_dict(critic_state_dict)

            self.logger.info("SL weights loaded and Critic FE initialized.")

            # c. åˆ›å»ºç®—æ³•å®ä¾‹ï¼Œå®ƒä¼šæ¥ç®¡æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ›´æ–°é€»è¾‘
            self.algorithm = PPOAlgorithm(self.config, self.actor, self.critic, self.logger)
            self.logger.info("PPOAlgorithm instance created.")

            # --- 6. ç­‰å¾… Replay Buffer æ•°æ® ---
            min_samples = self.config.get('min_sample_to_start_learner', 20000)
            
            self.logger.info(f"Waiting for Replay Buffer to have at least {min_samples} samples...")

            last_logged_size = -1

            should_exit = False
            
            # ç­‰å¾…ReplayBufferå¾ªç¯
            while True:  # æ— æ¡ä»¶å¾ªç¯ï¼Œå†…éƒ¨æ£€æŸ¥é€€å‡ºæ¡ä»¶
                if should_exit:
                    self.logger.info("should_exitæ ‡å¿—å·²è®¾ç½®ï¼Œé€€å‡ºç­‰å¾…å¾ªç¯")
                    break
                
                try:
                    # # å®‰å…¨æ£€æŸ¥shutdown_event
                    # if self.shutdown_event is not None:
                    #     try:
                    #         if self.shutdown_event.is_set():
                    #             self.logger.info("æ£€æµ‹åˆ°å…³é—­äº‹ä»¶å·²è®¾ç½®ï¼Œé€€å‡ºç­‰å¾…å¾ªç¯")
                    #             should_exit = True
                    #             break
                    #     except Exception as e_event:
                    #         self.logger.error(f"è®¿é—®shutdown_eventæ—¶å‡ºé”™: {e_event}ï¼Œå‡è®¾æœªè®¾ç½®")
                    
                    # è·å–å½“å‰bufferå¤§å°
                    current_buffer_size = self.replay_buffer.size()  # Calls _flush internally
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰€éœ€æ ·æœ¬æ•°
                    if current_buffer_size >= min_samples:
                        self.logger.info(f"å·²è¾¾åˆ°æ‰€éœ€æ ·æœ¬æ•°: {current_buffer_size}/{min_samples}")
                        break
                    
                    # å®šæœŸæ—¥å¿—è®°å½•
                    else:
                        self.logger.info(f"Replay Buffer size: {current_buffer_size}/{min_samples}. Waiting for more samples...")
                        time.sleep(5)
                        
                    
                except Exception as e_rb_wait:  # æ•è·ç¼“å†²åŒºæ£€æŸ¥è¿‡ç¨‹ä¸­çš„é”™è¯¯
                    self.logger.error(f"Error checking replay buffer size while waiting: {e_rb_wait}", exc_info=True)
                    # å®‰å…¨æ£€æŸ¥shutdown_event
                    try:
                        if self.shutdown_event is not None and self.shutdown_event.is_set():
                            self.logger.info("æ£€æµ‹åˆ°å…³é—­äº‹ä»¶å·²è®¾ç½®ï¼Œcatchå—ä¸­é€€å‡ºç­‰å¾…å¾ªç¯")
                            break
                    except Exception as e_event_catch:
                        self.logger.error(f"Catchå—ä¸­è®¿é—®shutdown_eventæ—¶å‡ºé”™: {e_event_catch}ï¼Œç»§ç»­ç­‰å¾…")
                    # å‡ºé”™æ—¶ç­‰å¾…è¾ƒé•¿æ—¶é—´
                    time.sleep(5)
            
            self.logger.info(f"Minimum samples ({min_samples}) reached in Replay Buffer. Starting training loop.")
            
            # --- 7. ä¸»è®­ç»ƒå¾ªç¯ ---
            cur_time_ckpt = time.time() 
            cur_time_log = time.time() 
            iterations = 0 
            steps_processed_since_log = 0 
                
            # ä¸»å¾ªç¯å¼€å§‹å‰å®‰å…¨æ£€æŸ¥shutdown_eventä»¥é¿å…å´©æºƒ
            shutdown_detected = False
            # try:
            #     if self.shutdown_event is not None and self.shutdown_event.is_set():
            #         shutdown_detected = True
            # except Exception as e_main_shutdown:
            #     self.logger.error(f"ä¸»å¾ªç¯å‰è®¿é—®shutdown_eventæ—¶å‡ºé”™: {e_main_shutdown}ï¼Œå‡è®¾æœªè®¾ç½®")
                
            while not shutdown_detected:  # ä½¿ç”¨å®‰å…¨æ ‡å¿—è€Œä¸æ˜¯ç›´æ¥è®¿é—®shutdown_event
                start_iter_time = time.time()

                # --- 7.0 æ›´æ–°æ¨¡å‹å‚æ•°çŠ¶æ€ (è§£å†») å’Œå­¦ä¹ ç‡ ---
                # a. æ›´æ–°æ¨¡å‹çš„å¯è®­ç»ƒçŠ¶æ€ (è§£å†»)
                # é¦–å…ˆè°ƒç”¨æ›´æ–°å†»ç»“çŠ¶æ€ï¼Œç„¶åæ ¹æ®å†»ç»“çŠ¶æ€è°ƒæ•´å­¦ä¹ ç‡
                self.algorithm.update_freezing_status(iterations)
                
                # b. æ ¹æ®å†»ç»“çŠ¶æ€è®¡ç®—å’Œæ›´æ–°å­¦ä¹ ç‡
                # æ³¨æ„ï¼šschedule_learning_rateå·²ç»ä¼˜åŒ–ï¼Œè€ƒè™‘äº†ç»„ä»¶è§£å†»æ—¶é—´
                self.algorithm.schedule_learning_rate(iterations)

                # --- 7.1. é‡‡æ ·å’Œæ•°æ®å‡†å¤‡ ---
                self.logger.debug(f"Iter {iterations}: Attempting to sample from Replay Buffer.")
                batch = None
                try:
                    batch = self.replay_buffer.sample(self.config['batch_size'])
                except Exception as e_sample:
                    self.logger.error(f"Iter {iterations}: Error during replay_buffer.sample(): {e_sample}", exc_info=True)
                    time.sleep(1)
                    continue

                if batch is None:
                    rb_size = self.replay_buffer.size() # This calls _flush
                    q_size = self.replay_buffer.queue.qsize() if hasattr(self.replay_buffer.queue, 'qsize') else 'N/A'
                    self.logger.info(f"Iter {iterations}: replay_buffer.sample() returned None. Buffer(timesteps): {rb_size}. InputQueue(episodes): {q_size}. Sleeping...")
                    time.sleep(self.config.get('learner_empty_batch_sleep_sec', 1.0)) 
                    continue
                self.logger.debug(f"Iter {iterations}: Batch sampled successfully.")
                
                # --- 7.2. PPO æ›´æ–° ---
                # d. è°ƒç”¨ç®—æ³•æ‰§è¡Œè®­ç»ƒæ­¥éª¤
                train_metrics = self.algorithm.train_on_batch(batch)

                if not train_metrics:  # å¦‚æœè®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡è¿™æ¬¡è¿­ä»£
                    self.logger.warning(f"Iter {iterations}: train_on_batch returned empty metrics. Skipping iteration.")
                    continue

                # --- 7.3. æ—¥å¿—è®°å½•å’Œ TensorBoard ---
                steps_processed_since_log += self.config['batch_size']
                current_time = time.time()
                log_interval = self.config.get('log_interval', 100)

                if iterations % log_interval == 0: 
                    time_since_log = current_time - cur_time_log if cur_time_log and iterations > 0 else (current_time - start_iter_time if iterations == 0 else 1.0)
                    iterations_per_sec = log_interval / time_since_log if time_since_log > 0 and iterations > 0 else \
                                        (1 / time_since_log if time_since_log > 0 and iterations == 0 else 0.0)
                    samples_per_sec = steps_processed_since_log / time_since_log if time_since_log > 0 else 0.0

                    buffer_size = self.replay_buffer.size()
                    buffer_stats_dict = self.replay_buffer.stats() if hasattr(self.replay_buffer, 'stats') and callable(getattr(self.replay_buffer, 'stats')) else {}
                    sample_in_rate = buffer_stats_dict.get('samples_in_per_second_smoothed', buffer_stats_dict.get('samples_in_per_second',0)) 
                    sample_out_rate = buffer_stats_dict.get('samples_out_per_second_smoothed', buffer_stats_dict.get('samples_out_per_second',0))
                      # è·å–å½“å‰å­¦ä¹ ç‡ä¿¡æ¯
                    log_lr_actor, log_lr_critic_fe, log_lr_critic_head = "N/A", "N/A", "N/A"
                    for pg in self.algorithm.optimizer.param_groups: 
                        lr_val_str = f"{pg['lr']:.2e}"
                        if pg.get('name') == 'actor': log_lr_actor = lr_val_str
                        if pg.get('name') == 'critic_feature_extractor': log_lr_critic_fe = lr_val_str
                        if pg.get('name') == 'critic_head': log_lr_critic_head = lr_val_str
                    
                    lr_log_str = f"LRs (A/CFE/CH): {log_lr_actor}/{log_lr_critic_fe}/{log_lr_critic_head}"
                    
                    # ä»è®­ç»ƒæŒ‡æ ‡ä¸­è·å–æŸå¤±å€¼
                    policy_loss_epoch_avg = train_metrics.get('policy_loss', 0.0)
                    critic_loss_epoch_avg = train_metrics.get('critic_loss', 0.0)
                    entropy_loss_epoch_avg = train_metrics.get('entropy_loss', 0.0)
                    total_loss_epoch_avg = train_metrics.get('total_loss', 0.0)
                    
                    log_msg = (
                        f"Iter: {iterations} | {lr_log_str} | "
                        f"Loss(Actual): {total_loss_epoch_avg:.4f} " 
                        f"(P_contrib: {policy_loss_epoch_avg:.4f}, C_contrib: {critic_loss_epoch_avg:.4f}, E_contrib: {entropy_loss_epoch_avg:.4f})"
                    )
                    self.logger.info(log_msg)
                    
                    # è®°å½•é‡è¦çš„è®­ç»ƒé‡Œç¨‹ç¢‘åˆ°ç»Ÿä¸€æ—¥å¿—ï¼ˆæ¯100æ¬¡è¿­ä»£æˆ–å…³é”®èŠ‚ç‚¹ï¼‰
                    if iterations % (log_interval * 10) == 0 or iterations in [10, 50, 100, 500, 1000]:
                        # ä¸»è¦æ—¥å¿—åªè®°å½•æ ¸å¿ƒè®­ç»ƒä¿¡æ¯
                        self.logger.info(f"ğŸ¯ TRAINING_MILESTONE | Iter: {iterations} | "
                                        f"TotalLoss: {total_loss_epoch_avg:.4f} | "
                                        f"PolicyLoss: {policy_loss_epoch_avg:.4f} | "
                                        f"CriticLoss: {critic_loss_epoch_avg:.4f} | "
                                        f"LearningRates: A={log_lr_actor} CFE={log_lr_critic_fe} CH={log_lr_critic_head}")

                    if self.writer:
                        self.writer.add_scalar('Loss/Total_Actual_Backward', total_loss_epoch_avg, iterations)
                        self.writer.add_scalar('Loss/Policy_Calculated', policy_loss_epoch_avg, iterations)
                        self.writer.add_scalar('Loss/Critic_Calculated', critic_loss_epoch_avg, iterations) 
                        self.writer.add_scalar('Loss/Entropy_Calculated', -entropy_loss_epoch_avg, iterations)
                        
                        # å­¦ä¹ ç‡è®°å½•ä¿ç•™åœ¨ä¸»TensorBoardä¸­ï¼Œå› ä¸ºè¿™æ˜¯æ ¸å¿ƒè®­ç»ƒæŒ‡æ ‡
                        for pg in self.algorithm.optimizer.param_groups:
                            group_name = pg.get('name', 'UnnamedGroup')
                            component_name_map = {
                                'actor': 'Actor',
                                'critic_feature_extractor': 'CriticFE', 
                                'critic_head': 'CriticHead'
                            }
                            tb_component_name = component_name_map.get(group_name)
                            if tb_component_name:
                                is_trainable_now = False
                                if tb_component_name == 'Actor':
                                    # Check if any actor parameter is trainable
                                    is_trainable_now = any(p.requires_grad for p in self.actor.parameters())
                                elif tb_component_name == 'CriticFE' and hasattr(self.critic, 'feature_extractor_obs'):
                                    # Check if critic FE obs is trainable
                                    is_trainable_now = any(p.requires_grad for p in self.critic.feature_extractor_obs.parameters())
                                elif tb_component_name == 'CriticHead':
                                    # Check if any component in critic head group is trainable
                                    is_trainable_now = False
                                    if hasattr(self.critic, 'critic_head_mlp'):
                                        is_trainable_now = any(p.requires_grad for p in self.critic.critic_head_mlp.parameters())
                                    if not is_trainable_now and hasattr(self.critic, 'feature_extractor_extra'):
                                        is_trainable_now = any(p.requires_grad for p in self.critic.feature_extractor_extra.parameters())
                                
                                if is_trainable_now:
                                    self.writer.add_scalar(f'LearningRate/{tb_component_name}', pg['lr'], iterations)
                        self.writer.flush()
                        
                        # æ‰€æœ‰ReplayBufferå’ŒPerformanceæŒ‡æ ‡éƒ½ç§»åˆ°è¯¦ç»†æ—¥å¿—çš„TensorBoardä¸­
                        if hasattr(self, 'detailed_writer') and self.detailed_writer:
                            self.detailed_writer.add_scalar('ReplayBuffer/SizeTimesteps', buffer_size, iterations)
                            self.detailed_writer.add_scalar('ReplayBuffer/RateIn', sample_in_rate, iterations)
                            self.detailed_writer.add_scalar('ReplayBuffer/RateOut', sample_out_rate, iterations)
                            self.detailed_writer.add_scalar('ReplayBuffer/QueueSizeEpisodes', buffer_stats_dict.get('queue_size',0), iterations)
                            self.detailed_writer.add_scalar('Performance/IterationsPerSecond_Learner', iterations_per_sec, iterations)
                            self.detailed_writer.add_scalar('Performance/SamplesPerSecond_Learner', samples_per_sec, iterations)
                            self.detailed_writer.add_scalar('Performance/BufferAndQueue', buffer_size + buffer_stats_dict.get('queue_size',0), iterations)
                            self.detailed_writer.flush()
                    cur_time_log = current_time
                    steps_processed_since_log = 0

                # --- 7.4 æ›´æ–° InferenceServer ä¸­çš„æ¨¡å‹ ---
                update_eval_model_interval = self.config.get('model_push_interval', 10)
                
                if iterations > 0 and iterations % update_eval_model_interval == 0: 
                    self.logger.debug(f"Iter {iterations}: Attempting to send updated model state to InferenceServer for its eval_model.")
                    try:
                        # å‘é€ actor çŠ¶æ€
                        actor_state_cpu = {k: v.cpu() for k, v in self.actor.state_dict().items()}
                        critic_state_cpu = {k: v.cpu() for k, v in self.critic.state_dict().items()}

                        if self.inference_server_cmd_queue:
                            # æ·»åŠ è¶…æ—¶æœºåˆ¶ï¼Œé¿å…å› é˜Ÿåˆ—æ»¡å¯¼è‡´é˜»å¡
                            queue_put_timeout = self.config.get('queue_put_timeout_sec', 5.0)
                            
                            # å°è¯•æ¨é€actoræ¨¡å‹ï¼Œæ·»åŠ è¶…æ—¶
                            try:
                                self.inference_server_cmd_queue.put(("UPDATE_ACTOR_MODEL", actor_state_cpu), 
                                                                  timeout=queue_put_timeout)
                                self.logger.debug(f"Iter {iterations}: Command 'UPDATE_ACTOR_MODEL' sent to InferenceServer.")
                            except Exception as e_put_actor:
                                self.logger.error(f"Iter {iterations}: Failed to send 'UPDATE_ACTOR_MODEL' command: {e_put_actor}. "
                                                 f"Queue might be full or InferenceServer unresponsive.")
                            
                            # å°è¯•æ¨é€criticæ¨¡å‹ï¼Œæ·»åŠ è¶…æ—¶
                            try:
                                self.inference_server_cmd_queue.put(("UPDATE_CRITIC_MODEL", critic_state_cpu),
                                                                  timeout=queue_put_timeout)
                                self.logger.debug(f"Iter {iterations}: Command 'UPDATE_CRITIC_MODEL' sent to InferenceServer.")
                            except Exception as e_put_critic:
                                self.logger.error(f"Iter {iterations}: Failed to send 'UPDATE_CRITIC_MODEL' command: {e_put_critic}. "
                                                 f"Queue might be full or InferenceServer unresponsive.")
                        else:
                            self.logger.error(f"Iter {iterations}: Cannot send model update to InferenceServer: command queue is None.")

                    except Exception as e_update_eval_model:
                        self.logger.error(f"Iter {iterations}: Failed to send model update commands: {e_update_eval_model}", exc_info=True)

                # --- 7.5. ä¿å­˜æ£€æŸ¥ç‚¹ ---
                t_now_ckpt = time.time()
                ckpt_interval_sec = self.config.get('ckpt_save_interval_seconds', 600)
                ckpt_dir = learner_log_paths.get('checkpoint_dir', 'log/model')  # ä½¿ç”¨æ–°çš„è·¯å¾„

                if iterations > 0 and t_now_ckpt - cur_time_ckpt > ckpt_interval_sec : 
                    os.makedirs(ckpt_dir, exist_ok=True)
                    path = os.path.join(ckpt_dir, f'model_iter_{iterations}.pt')
                    self.logger.info(f"Saving checkpoint at iteration {iterations} to {path}...")
                    try:
                        serializable_config = {k: v for k, v in self.config.items() if isinstance(v, (int, float, str, bool, list, dict, type(None)))}
                        torch.save({
                            'iteration': iterations,
                            'actor_state_dict': self.actor.state_dict(),
                            'critic_state_dict': self.critic.state_dict(),
                            'optimizer_state_dict': self.algorithm.optimizer.state_dict(),
                            'config': serializable_config 
                        }, path)
                        cur_time_ckpt = t_now_ckpt
                    except Exception as e:
                        self.logger.error(f"Failed to save checkpoint at iteration {iterations}: {e}", exc_info=True)

                iterations += 1
                
                # å®‰å…¨æ£€æŸ¥shutdown_eventï¼Œé¿å…ç›´æ¥è®¿é—®å¯¼è‡´å´©æºƒ
                # try:
                #     if self.shutdown_event is not None and self.shutdown_event.is_set():
                #         self.logger.info(f"åœ¨ç¬¬ {iterations} æ¬¡è¿­ä»£ç»“æŸæ—¶æ£€æµ‹åˆ°å…³é—­äº‹ä»¶å·²è®¾ç½®ï¼Œé€€å‡ºä¸»å¾ªç¯")
                #         shutdown_detected = True
                #         break
                # except Exception as e_iter_shutdown:
                #     # å¦‚æœè®¿é—®shutdown_eventå‡ºé”™ï¼Œç»§ç»­æ‰§è¡Œä½†è®°å½•é”™è¯¯
                #     self.logger.error(f"ç¬¬ {iterations} æ¬¡è¿­ä»£æ£€æŸ¥shutdown_eventæ—¶å‡ºé”™: {e_iter_shutdown}")
                #     # æ¯100æ¬¡è¿­ä»£æ£€æŸ¥ä¸€æ¬¡self.shutting_downä½œä¸ºå¤‡ç”¨æ–¹å¼
                #     if iterations % 100 == 0 and self.shutting_down:
                #         self.logger.info(f"é€šè¿‡self.shutting_downæ£€æµ‹åˆ°å…³é—­ä¿¡å·ï¼Œé€€å‡ºä¸»å¾ªç¯")
                #         break
                
            self.logger.info("Learner training loop finished (or was interrupted).")
            if self.writer:
                self.writer.close()
        
        except KeyboardInterrupt: 
            self.logger.warning(f"Learner {self.name} (PID: {os.getpid()}) caught KeyboardInterrupt in main try block.")
            if not self.shutting_down: # å¦‚æœä¿¡å·å¤„ç†å™¨è¿˜æœªè¿è¡Œ
                self._signal_handler(signal.SIGINT, None)
        except Exception as e_main_run:
            self.logger.critical(f"Learner {self.name} (PID: {os.getpid()}) encountered an unhandled exception in main run logic: {e_main_run}", exc_info=True)
        finally:
            self.logger.info(f"Learner {self.name} (PID: {os.getpid()}) entering finally block of run method.")
            if not self.shutting_down: 
                self.shutting_down = True 
                self.logger.info("Learner run method finished or aborted without external signal. Performing final cleanup...")
                # Learner ä¸å†æ‹¥æœ‰ ModelPoolServerï¼Œæ‰€ä»¥ä¸åœ¨è¿™é‡Œæ¸…ç†å®ƒ
                if self.writer:
                    try: self.writer.close()
                    except Exception as e_final_writer: self.logger.error(f"Error during final TensorBoard writer close: {e_final_writer}", exc_info=True)
                
                # å…³é—­è¯¦ç»†æ—¥å¿—çš„writer
                if hasattr(self, 'detailed_writer') and self.detailed_writer:
                    try: self.detailed_writer.close()
                    except Exception as e_final_detailed_writer: self.logger.error(f"Error during final detailed TensorBoard writer close: {e_final_detailed_writer}", exc_info=True)
            
            # æ— è®ºå¦‚ä½•éƒ½è¦æ¸…ç†å…±äº«å†…å­˜å’ŒCUDAèµ„æºï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
            self.logger.info("æœ€ç»ˆæ¸…ç†æ‰€æœ‰å…±äº«å†…å­˜å’ŒCUDAèµ„æº...")
            try:
                self._cleanup_resources()
            except Exception as e_cleanup:
                self.logger.error(f"æœ€ç»ˆèµ„æºæ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e_cleanup}", exc_info=True)
                
            try:
                signal.signal(signal.SIGINT, original_sigint_handler)
                signal.signal(signal.SIGTERM, original_sigterm_handler)
            except Exception as e_restore_signal:
                 self.logger.warning(f"Could not restore original signal handlers in Learner: {e_restore_signal}")
            self.logger.info(f"Learner {self.name} (PID: {os.getpid()}) run method fully completed.")
    
    def _cleanup_resources(self):
        """æ¸…ç†å…±äº«å†…å­˜å’ŒCUDAèµ„æºï¼Œé˜²æ­¢å†…å­˜æ³„æ¼"""
        if self.logger:
            self.logger.info("å¼€å§‹æ¸…ç†èµ„æºä»¥é˜²æ­¢å†…å­˜æ³„æ¼...")
        else:
            print("å¼€å§‹æ¸…ç†èµ„æºä»¥é˜²æ­¢å†…å­˜æ³„æ¼...")
            
        # æ¸…ç†æ¨¡å‹èµ„æº
        if hasattr(self, 'actor') and self.actor is not None:
            try:
                del self.actor
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                if self.logger:
                    self.logger.info("Actoræ¨¡å‹èµ„æºå·²æ¸…ç†")
            except Exception as e:
                if self.logger:
                    self.logger.error(f"æ¸…ç†Actoræ¨¡å‹èµ„æºæ—¶å‡ºé”™: {e}", exc_info=True)
                else:
                    print(f"æ¸…ç†Actoræ¨¡å‹èµ„æºæ—¶å‡ºé”™: {e}")
        
        if hasattr(self, 'critic') and self.critic is not None:
            try:
                del self.critic
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                if self.logger:
                    self.logger.info("Criticæ¨¡å‹èµ„æºå·²æ¸…ç†")
            except Exception as e:
                if self.logger:
                    self.logger.error(f"æ¸…ç†Criticæ¨¡å‹èµ„æºæ—¶å‡ºé”™: {e}", exc_info=True)
                else:
                    print(f"æ¸…ç†Criticæ¨¡å‹èµ„æºæ—¶å‡ºé”™: {e}")
        
        # æ¸…ç†ä¼˜åŒ–å™¨èµ„æº
        if hasattr(self, 'algorithm') and hasattr(self.algorithm, 'optimizer') and self.algorithm.optimizer is not None:
            try:
                del self.algorithm.optimizer
                if self.logger:
                    self.logger.info("ä¼˜åŒ–å™¨èµ„æºå·²æ¸…ç†")
            except Exception as e:
                if self.logger:
                    self.logger.error(f"æ¸…ç†ä¼˜åŒ–å™¨èµ„æºæ—¶å‡ºé”™: {e}", exc_info=True)
                else:
                    print(f"æ¸…ç†ä¼˜åŒ–å™¨èµ„æºæ—¶å‡ºé”™: {e}")
        
        if hasattr(self, 'algorithm') and self.algorithm is not None:
            try:
                del self.algorithm
                if self.logger:
                    self.logger.info("ç®—æ³•å®ä¾‹èµ„æºå·²æ¸…ç†")
            except Exception as e:
                if self.logger:
                    self.logger.error(f"æ¸…ç†ç®—æ³•å®ä¾‹èµ„æºæ—¶å‡ºé”™: {e}", exc_info=True)
                else:
                    print(f"æ¸…ç†ç®—æ³•å®ä¾‹èµ„æºæ—¶å‡ºé”™: {e}")
        
        # å¼ºåˆ¶è¿›è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶
        import gc
        collected = gc.collect()
        if self.logger:
            self.logger.info(f"åƒåœ¾å›æ”¶å™¨å›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
        else:
            print(f"åƒåœ¾å›æ”¶å™¨å›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            if self.logger:
                self.logger.info("CUDAç¼“å­˜å·²æ¸…ç©º")
            else:
                print("CUDAç¼“å­˜å·²æ¸…ç©º")
                
        if self.logger:
            self.logger.info("èµ„æºæ¸…ç†å®Œæˆ")
        else:
            print("èµ„æºæ¸…ç†å®Œæˆ")