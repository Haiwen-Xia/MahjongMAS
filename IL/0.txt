这个是我用于建模麻将agent所采用的网络的草稿；
请基于它进行补全，并回答以下问题：
1.这里面的三种attention都可以基于nn.MultiHeadAttention实现吗（带残差连接，layernorm）
2.时间序列显然需要padding;padding的生成应该在dataset还是dataloader？mask需要传到输入里吗？
能不能给一个mask的例子？（包含它的作用效果）
3.该框架最后要用到强化学习中，因此轨迹长度的差距可能较大而我并不知道训练actor/critic一步step所需要收集的数据量级，padding的最佳方式是什么？（到最长还是分桶）
import torch.nn as nn
import torch
from omegaconf import OmegaConf
from conv import ConvBlock, ResidualBlock
class TimeNet(nn.Module):
    #args 应该能直接传入hydra超参数， info 是input的信息？
    def __init__(self, 
                 history_feature_dim, 
                 state_feature_shape, 
                 output_dim, 
                 input_info = {}, 
                 args = {}):
        super(TimeNet, self).__init__() #* super返回一个TimeNet父类的代理
        self.history_feature_dim = history_feature_dim # default 95
        self.state_feature_shape = state_feature_shape # default B, (4,4,9)
        self.hid_dim = args.get('hid_dim', 512)
        self.dropout = args.get('dropout', 0.1)
        
        self.embed = nn.Linear(history_feature_dim, self.hid_dim)
        
        self.pos_encoding = ... # sin encoding
        self.self_attention = #* 对历史信息进行self attention, 需要常规的残差连接
        self.h_query_attention = #* 以历史信息为query进行attention
        self.s_query_attention = #* 以当前状态为query进行attention
        
        self.state_extractor = nn.Sequential(
            ConvBlock(state_feature_shape[0], 128),
            ResidualBlock(128, 128),
            ResidualBlock(128, 64),
            nn.Flatten(),
            nn.Linear(4 * 9 * 64, self.hid_dim)
        )
        
        self.fc = nn.Sequential(
            nn.Linear(self.hid_dim , self.hid_dim/2),
            nn.ReLU(),
            nn.Linear(self.hid_dim/2, output_dim)
        )
    def forward(self, input_dict):
        history = input_dict['input']['history'].float()
        state = input_dict['input']['hand'].float()
        
        history = self.embed(history) #* 线性映射到hid_dim维度
        history = history + self.pos_encoding(history)
        history = self.self_attention(history)
        
        state = self.state_extractor(state)
        
        history = self.h_query_attention(history, state) #* 可能要多次
        
        state = self.s_query_attention(state, history) #* 可能要多次,和h_query混合
        
        action_logits = self.fc(state)
        
        action_mask = input_dict['obs']['action_mask'].float()
        inf_mask = torch.clamp(torch.log(action_mask), -1e38, 1e38)
        
        return action_logits + inf_mask

query 似乎就算不pad问题也不会太大...?
前提是self attention要一直带mask?